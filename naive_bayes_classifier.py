# -*- coding: utf-8 -*-
"""Naive Bayes Classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1908Zfm-6WWxTQxEs3i5-mUKIXTmp9lfy

#Naive Bayes Classifier

# read dataset2
"""

import os
traindata=[]
label=[]
for i in os.listdir("E:/dataset2/train/comp.sys.ibm.pc.hardware"):
    try:
        txt=" ".join([i.strip().lower() for i in open("E:/dataset2/train/comp.sys.ibm.pc.hardware/"+i,encoding="utf-8").readlines()])
        traindata.append(txt)
        label.append(1)
    except:
        pass

for i in os.listdir("E:/dataset2/train/sci.electronics"):
    try:
        txt=" ".join([i.strip().lower() for i in open("E:/dataset2/train/sci.electronics/"+i,encoding="utf-8").readlines()])
        traindata.append(txt)
        label.append(0)
    except:
        pass

import os
testdata=[]
testlabel=[]
for i in os.listdir("E:/dataset2/test/comp.sys.ibm.pc.hardware"):
    try:
        txt=" ".join([i.strip().lower() for i in open("E:/dataset2/test/comp.sys.ibm.pc.hardware/"+i,encoding="utf-8").readlines()])
        testdata.append(txt)
        testlabel.append(1)
    except:
        pass

for i in os.listdir("E:/dataset2/test/sci.electronics"):
    try:
        txt=" ".join([i.strip().lower() for i in open("E:/dataset2/test/sci.electronics/"+i,encoding="utf-8").readlines()])
        testdata.append(txt)
        testlabel.append(0)
    except:
        pass

import numpy as np
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB


def textParse(bigString):
    try:
        bigString=bigString[bigString.index("lines:"):]
    except:
        pass
    #Divide words by spaces
    listOfTokens = bigString.split()
    #Remove useless words with word length less than 2
    return [tok.lower() for tok in listOfTokens if len(tok)>2]

#Remove duplicate elements in the list and return as a list
def createVocaList(dataSet):
    vocabSet = set({})
    #Remove duplicate elements, take the union
    for document in dataSet:
        vocabSet = vocabSet | set(document)
    return list(vocabSet)

#Count the number of times each document appears in the word list and return it in the form of a list
def setOfWordsToVec(vocabList,inputSet):
    #Create a 0 vector whose length is the total number of words
    returnVec = [0]*len(vocabList)
    #Count the number of corresponding words
    for word in inputSet:
        if word in vocabList:
            returnVec[vocabList.index(word)] += 1
    return returnVec

from tqdm import tqdm
for i in range(len(traindata)):
    traindata[i]=textParse(traindata[i])

for i in range(len(testdata)):
    testdata[i]=textParse(testdata[i])

vocabList = createVocaList(traindata+testdata) 

for i in tqdm(range(len(traindata))):
    traindata[i]=setOfWordsToVec(vocabList, traindata[i])

for i in tqdm(range(len(testdata))):
    testdata[i]=setOfWordsToVec(vocabList, testdata[i])

pSpam=MultinomialNB()
pSpam.fit(traindata,label)
#test data
y_pred=pSpam.predict(testdata)

print("accuracyï¼š%f%%"%(accuracy_score(y_pred, testlabel)*100))